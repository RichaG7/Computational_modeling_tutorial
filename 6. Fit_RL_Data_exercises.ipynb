{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning model used to generate choices is a simple Rescorla-Wagner (Rescorla & Wagner, 1972) model.\n",
    "\n",
    "$$ Q(action, trial) = Q(action, trial - 1) + \\alpha*prediction_{error}$$\n",
    "$$ prediction_{error} = r(trial-1) - Q(action, trial - 1)$$\n",
    "$$ 0 <= \\alpha <= 1 $$\n",
    "\n",
    "In the previous scripts, you have already read about the RL model and also already came across the softmax choice rule (recall the orange juice/coffee example). Now, we are going to use the softmax function again to generate choice probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(q_values, beta):\n",
    "    # Inputs:\n",
    "    #       q_values: value of different actions, often just 2\n",
    "    #       beta: inverse temperature, also known as exploitation parameter\n",
    "    # Outputs:\n",
    "    #       choice_probs: model's predicted probability of actions\n",
    "\n",
    "    # Numerator represents value of utility of single choice\n",
    "    numerator = np.exp(np.multiply(q_values, beta))\n",
    "\n",
    "    # Denominator represents sum of total value of utilities\n",
    "    denominator = np.sum(np.exp(np.multiply(q_values, beta)))\n",
    "    \n",
    "    # Outputs\n",
    "    choice_probs = numerator / denominator\n",
    "        \n",
    "    return choice_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "The function task_parameters generates the experimental data that an agent will receive. It defines the volatility of the switching, the block number, the trial number, the probability that the left and right levers give rewards, and whether they actually would give a reward (random sample drawn from that probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_parameters(volatility = 'low'): \n",
    "    \n",
    "    # Generate dataframe to store simulated results\n",
    "    sim_exp = pd.DataFrame(columns = ['volatility', 'block', 'block_trial', 'exp_trial', \n",
    "                                       'prob_reward_left', 'prob_reward_right', 'reward_left', 'reward_right'])\n",
    "    \n",
    "    # Trial sequence\n",
    "    block_seq = [[17, 15, 19, 18, 15, 17, 19, 15], # high volatility\n",
    "                [35, 30, 35, 35]] # low volatility\n",
    "    reward_probs_left = [.7, .3] \n",
    "    reward_probs_right = [.3, .7] \n",
    "\n",
    "    if volatility == 'low':\n",
    "        task_blocks = block_seq[1]\n",
    "        block_prob_reward_left = np.array(reward_probs_left*(np.int(len(task_blocks)/2)))\n",
    "        block_prob_reward_right = np.array(reward_probs_right*(np.int(len(task_blocks)/2)))\n",
    "    else: \n",
    "        task_blocks = block_seq[0]\n",
    "        block_prob_reward_left = np.array(reward_probs_left*(np.int(len(task_blocks)/2)))\n",
    "        block_prob_reward_right = np.array(reward_probs_right*(np.int(len(task_blocks)/2)))\n",
    "\n",
    "    # generate trials\n",
    "    trial = 0\n",
    "    for block in range(len(task_blocks)): \n",
    "        block_trial = 0\n",
    "        # Loop through trials of the block\n",
    "        for block_trial in range(task_blocks[block]):\n",
    "            trial += 1\n",
    "            block_trial += 1\n",
    "            \n",
    "            # Generate reward for both choices\n",
    "            prob_reward_left = block_prob_reward_left[block] # pulls correct probability for left\n",
    "            prob_reward_right = block_prob_reward_right[block] # pulls correct probability for right\n",
    "            \n",
    "            # Left choice\n",
    "            if np.random.rand() <= prob_reward_left:\n",
    "                reward_left = 1\n",
    "            else: \n",
    "                reward_left = 0\n",
    "                \n",
    "            # Right choice\n",
    "            if np.random.rand() <= prob_reward_right:\n",
    "                reward_right = 1\n",
    "            else: \n",
    "                reward_right = 0\n",
    "                \n",
    "            # Store results\n",
    "            trial_data = pd.DataFrame([[volatility, block+1, block_trial, trial, \n",
    "                                        prob_reward_left, prob_reward_right, reward_left, reward_right]], \n",
    "                                     columns = ['volatility', 'block', 'block_trial', 'exp_trial', \n",
    "                                       'prob_reward_left', 'prob_reward_right', 'reward_left', 'reward_right'])\n",
    "            sim_exp = sim_exp.append(trial_data).reset_index(drop=True)\n",
    "            \n",
    "    return sim_exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "With the simulated task parameters from the task_parameter function, you can now generate choices with rl_simulate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_simulate(params, sim_exp): \n",
    "    data = sim_exp.copy()\n",
    "    # Initialize parameters\n",
    "    # two alphas? (positive vs negative pes), stickiness? (likelihood of just repeating past action)\n",
    "    # two-step modeling\n",
    "    alpha = params[0] # learning param\n",
    "    beta = params[1] # exploitation param\n",
    "    q_values = [.5, .5] # q-values for left and right choices start at .5\n",
    "    \n",
    "    # Loop over rows in data\n",
    "    for index, row in data.iterrows(): # loop over rows, slow but easy to understand and index\n",
    "        \n",
    "        # Compute probability of chosing left or right\n",
    "        choice_probs = softmax(q_values, beta) \n",
    "        \n",
    "        # Make choice weighted by probabilities\n",
    "        # probabilities given by left, right\n",
    "        if np.random.rand() <= choice_probs[0]:\n",
    "            choice = 'left'\n",
    "            choice_num = 0\n",
    "        else:\n",
    "            choice = 'right'\n",
    "            choice_num = 1\n",
    "\n",
    "                    \n",
    "        # Store (keep above update)      \n",
    "        data.at[index, 'choice'] = choice # useful way to append single values\n",
    "        data.at[index, 'choice_num'] = choice_num\n",
    "        data.at[index, 'prob_left'] = choice_probs[0]\n",
    "        data.at[index, 'prob_right'] = choice_probs[1]\n",
    "        data.at[index, 'q_left'] = q_values[0]\n",
    "        data.at[index, 'q_right'] = q_values[1]\n",
    "        \n",
    "        # Update q_values (to be used on next trial)\n",
    "        # All the magic happens here\n",
    "        if choice == 'left':\n",
    "            pe = row['reward_left'] - q_values[choice_num] # compute pe = reward - q_value['choice']\n",
    "            q_values[choice_num] = q_values[choice_num] + alpha*pe # update value\n",
    "        else:\n",
    "            pe = row['reward_right'] - q_values[choice_num] # compute pe\n",
    "            q_values[choice_num] = q_values[choice_num] + alpha*pe # update value\n",
    "                \n",
    "    # Add final information\n",
    "    data['alpha'] = alpha\n",
    "    data['beta'] = beta\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Now you have a function to simulate task parameters and a function to simulate choice. Using these two functions, visualize the probability of getting a reward for choosing the right lever and the simulated choice. \n",
    "You can also add a running average to visualize how the simulated subject behaved on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "\n",
    "Adjust rl_simulate to rl_simulate_fit. The rl_simulate_fit function returns the negative loglikelihood based on the alpha and beta parameters and some input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_simulate_fit(params, data): \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                 \n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "\n",
    "1. Define your true parameters\n",
    "2. simulate task parameters\n",
    "3. simulate data\n",
    "4. using the function you defined in exercise 2, fit the model to the simulated data to obtain the best parameters\n",
    "5. Repeat the model fitting 20 times and store results in fit_results\n",
    "\n",
    "You can draw inspiration from notebook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
